x-spark-common: &spark-common
    build:
      context: ./spark
      dockerfile: Dockerfile

services:
  minio:
    image: minio/minio:RELEASE.2025-06-13T11-33-47Z
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - ../data/minio:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 3s
      timeout: 3s
      retries: 10
      start_period: 5s
  init-minio:
    image: minio/mc:RELEASE.2025-05-21T01-59-54Z
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ../downloads/olist/:/mnt/downloads/olist/
    entrypoint: >
      sh -c '
        mc alias set local http://minio:9000 minioadmin minioadmin &&
        mc mb --ignore-existing local/warehouse-dev/bronze &&
        mc cp /mnt/downloads/olist/*.csv local/warehouse-dev/bronze || true &&
        echo "init-minio ready; keeping container alive" &&
        tail -f /dev/null
      '
  
  spark-master:
      <<: *spark-common
      hostname: spark-master
      ports:
        - "8080:8080"
        - "7077:7077"
      volumes:
        - ../configs/spark:/opt/spark/conf
      command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    
  spark-worker:
    <<: *spark-common
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      SPARK_WORKER_MEMORY: 4g
      SPARK_WORKER_CORES: 2
    volumes:
      - ../configs/spark:/opt/spark/conf
    command: >
      bash -c "
        sleep 10 &&
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "
  spark-client:
    <<: *spark-common
    hostname: spark-client
    depends_on:
      - spark-master
    ports:
      - "10000:10000"
      - "4041:4041"
    volumes:
      - ../configs/spark:/opt/spark/conf
      - ../notebooks/spark:/opt/spark/work-dir/notebooks
      - ../jobs:/opt/spark/work-dir/jobs
      - ./spark/logs:/opt/spark/logs

    working_dir: /opt/spark/work-dir
    environment:
      - SPARK_NO_DAEMONIZE=1  # run foreground

    # command: tail -f /dev/null
    command: >
      /opt/spark/sbin/start-thriftserver.sh \
        --master spark://spark-master:7077 --executor-memory 3g \
        --executor-cores 1 \
        --driver-memory 2g &&
      spark-sql -e "CREATE NAMESPACE IF NOT EXISTS warehouse_dev.default;"
  spark-init:
    <<: *spark-common
    depends_on:
      - spark-master
      - rest-catalog
    volumes:
      - ../configs/spark:/opt/spark/conf
    entrypoint: >
      bash -c "
        echo 'Waiting for master/catalogâ€¦';
        sleep 10;
        spark-sql -e \"CREATE NAMESPACE IF NOT EXISTS warehouse_dev.default;\"
      "
      
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes: 
      # - "../data/postgres:/var/lib/postgresql/data"
      - "./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  rest-catalog:
    # image: catalog:ice1.9.1-psql16
    build:
      context: ./iceberg-rest
      dockerfile: Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
    ports: ["8181:8181"]

    environment:
      CATALOG_URI: jdbc:postgresql://postgres:5432/iceberg
      CATALOG_JDBC_USER: iceberg
      CATALOG_JDBC_PASSWORD: iceberg
      CATALOG_WAREHOUSE: s3://warehouse-dev/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO

      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: true
      
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1