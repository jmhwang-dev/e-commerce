x-spark-common: &spark-common
  image: ghcr.io/jmhwang-dev/warehouse:sp3.5.6-ice1.9.1

services:
  spark-client:
    <<: *spark-common
    container_name: spark-client
    network_mode: host
    
    depends_on:
      - spark-master-cdc
      - spark-master-etl
      - spark-history
      
    volumes:
      - ./configs/spark:/opt/spark/conf
      - ./configs/kafka:/configs/kafka:ro
      - ./notebooks/spark:/opt/spark/work-dir
      - ./jobs/:/jobs/
      - ./src/:/mnt/src
      - ./logs/spark/:/opt/spark/logs/
      # JMX 모니터링이 필요 없다면 아래 두 줄은 주석 처리해도 됩니다.
      - ./downloads/jmx_exporter:/mnt/jmx_exporter/
      - ./configs/jmx/spark_driver.yml:/mnt/configs/jmx/spark_driver.yml

    working_dir: /
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYTHONPATH=/mnt/src
      
      # [매우 중요] 드라이버가 외부(RPi/Desktop) 워커에게 알려줄 자신의 주소
      - SPARK_LOCAL_HOSTNAME=192.168.45.192
      - spark.driver.bindAddress=0.0.0.0

    # 리소스 제한 (Mini-PC 16GB 중 3GB 할당 -> 아주 적절함)
    # CDC Driver 4개 + Batch Driver 1개 동시에 띄우기에 충분합니다.
    mem_limit: 3g  
    cpus: 1.0
 
    command: tail -f /dev/null

  spark-master-cdc:
    <<: *spark-common
    hostname: spark-master-cdc
    container_name: spark-master-cdc
    ports:
      - "8080:8080"  # Web UI (CDC는 기본 8080 사용)
      - "7077:7077"  # Master Port (RPi 워커들이 여기로 연결)
    volumes:
      - ./configs/spark:/opt/spark/conf
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    depends_on:
      - spark-history

  # [Master 2] Batch Layer (ETL)
  # - Service Port: 7078
  # - Web UI: 8081
  spark-master-etl:
    <<: *spark-common
    hostname: spark-master-etl
    container_name: spark-master-etl
    ports:
      - "8081:8080"  # Web UI (호스트 8081 -> 컨테이너 8080)
      - "7078:7077"  # Master Port (데스크탑 워커들이 여기로 연결)
    volumes:
      - ./configs/spark:/opt/spark/conf
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    depends_on:
      - spark-history

  # [History Server]
  # - Web UI: 18080 (CDC 마스터와 겹치지 않게 주의!)
  spark-history:
    <<: *spark-common
    container_name: spark-history
    ports:
      - "18080:18080"
    volumes:
      - ./configs/spark:/opt/spark/conf
      # 중요: 모든 마스터와 워커가 로그를 여기로 쌓아야 보입니다.
      # 실제로는 MinIO나 HDFS 경로를 쓰는 게 좋지만, 로컬 테스트에선 공유 볼륨 필요
      - ./logs/spark/:/opt/spark/logs/
      - ./data/spark-events:/opt/spark/events # spark-defaults.conf의 eventLog.dir 경로와 일치해야 함
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer

  # [Init Job]
  init-spark:
    <<: *spark-common
    container_name: init-spark
    depends_on:
      - spark-master-cdc # 둘 중 아무거나 의존해도 됨
      - spark-master-etl
    volumes:
      - ./configs/spark:/opt/spark/conf
    entrypoint: >
      bash -c "
        echo 'Waiting for masters...';
        sleep 10;
        # 로컬 모드로 실행하여 네임스페이스 생성 (Master 연결 불필요)
        spark-sql --master local[*] -e \"CREATE NAMESPACE IF NOT EXISTS warehousedev.default;\"
      "