# Spark 설정 우선순위: SparkConf 코드 > spark-submit 플래그 > spark-defaults.conf > 환경 변수 (SPARK_...) > 기본값
spark.eventLog.enabled true
spark.eventLog.dir file:///opt/spark/logs/
spark.history.fs.logDirectory file:///opt/spark/logs/
spark.history.ui.port 18080

spark.sql.session.timeZone UTC

# checkpoint 쓰기 지연 관련
spark.hadoop.fs.s3a.fast.upload true
spark.hadoop.fs.s3a.multipart.size 10m
spark.hadoop.fs.s3a.connection.timeout 60000
spark.hadoop.fs.s3a.retry.limit 20

# 체크포인트 정리
# spark.cleaner.referenceTracking.cleanCheckpoints true
# spark.cleaner.periodicGC.interval 1min

# RocksDB State Store 활성화
spark.sql.streaming.stateStore.providerClass    org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider

# shuffle
spark.sql.shuffle.partitions 5

# 운영, 대량 처리시 AQE 사용. (AQE는 스트리밍 쿼리에서는 무시)
# spark.sql.adaptive.enabled                          false
# spark.sql.adaptive.coalescePartitions.enabled       true
# spark.sql.adaptive.coalescePartitions.minPartitionNum 1
# spark.sql.adaptive.coalescePartitions.initialPartitionNum 20
# spark.sql.adaptive.skewJoin.enabled                 true
# spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes 10MB

# spark.streaming.stopGracefullyOnShutdown true

#####
spark.metrics.appStatusSource.enabled true
#####

spark.sql.streaming.metricsEnabled true
spark.driver.memory 3g

# deploy-mode가 client일 때는 spark.driver.cores는 무시
spark.driver.cores 1

# 드라이버가 수집하는 결과 데이터의 최대 크기. 메모리 부족 방지.
spark.driver.maxResultSize 2g

# spark-submit 명령어의 플래그에서는 --num-executors
spark.executor.instances 1

spark.executor.cores 2
spark.executor.memory 2g
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 6
spark.executor.allowMixedArchitectures true

spark.sql.extensions                                    org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# REST catalog
spark.sql.catalog.warehousedev                             org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.warehousedev.type                        rest
spark.sql.catalog.warehousedev.uri                         http://192.168.45.191:8181
spark.sql.catalog.warehousedev.io-impl                     org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.warehousedev.warehouse                   s3://warehousedev
spark.sql.catalog.warehousedev.s3.endpoint                 http://192.168.45.191:9000
spark.sql.catalog.warehousedev.s3.path-style-access        true
spark.sql.catalog.warehousedev.s3.region                   us-east-1
spark.sql.catalog.warehousedev.s3.access-key-id            minioadmin
spark.sql.catalog.warehousedev.s3.secret-access-key        minioadmin

# Hive Metastore 비활성화 및 REST Catalog 전용 사용
spark.sql.defaultCatalog                                    warehousedev
spark.sql.catalogImplementation                             in-memory


## 익스큐터용 (모든 모드에서 적용됨)
# spark.executor.extraJavaOptions                  -Daws.region=us-east-1
spark.executorEnv.AWS_REGION                     us-east-1
spark.executorEnv.AWS_ACCESS_KEY_ID              minioadmin
spark.executorEnv.AWS_SECRET_ACCESS_KEY          minioadmin

# 드라이버용 (client 모드에서만 적용. 그렇지 않으면 환경변수 지정 필요)
# spark.driver.extraJavaOptions                    -Daws.region=us-east-1

# 드라이버용 (cluster 모드에서만 적용)
# spark.driverEnv.AWS_REGION                          us-east-1


spark.hadoop.fs.s3a.endpoint                    http://192.168.45.191:9000
spark.hadoop.fs.s3a.path.style.access          true
spark.hadoop.fs.s3a.access.key                 minioadmin
spark.hadoop.fs.s3a.secret.key                 minioadmin
spark.hadoop.fs.s3a.connection.ssl.enabled     false
spark.hadoop.fs.s3a.impl                       org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3.impl                       org.apache.hadoop.fs.s3a.S3AFileSystem