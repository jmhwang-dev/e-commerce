# Spark 설정 우선순위: SparkConf 코드 > spark-submit 플래그 > spark-defaults.conf > 환경 변수 (SPARK_...) > 기본값

# =========================================================
# 1. 공통 시스템 설정
# =========================================================
spark.driver.host              192.168.45.190
spark.driver.bindAddress       0.0.0.0
spark.driver.port              7003
spark.driver.blockManager.port 7004

spark.sql.session.timeZone       UTC
spark.driver.maxResultSize       2g
spark.sql.shuffle.partitions     5

# =========================================================
# 2. 이벤트 로그 (MinIO 중앙화)
# =========================================================
spark.eventLog.enabled           true
spark.eventLog.dir               s3a://spark-events/logs
spark.history.fs.logDirectory    s3a://spark-events/logs

# =========================================================
# 3. 모니터링 (Grafana/Prometheus 연동
# =========================================================
spark.metrics.appStatusSource.enabled true
spark.sql.streaming.metricsEnabled    true

# =========================================================
# 4. 스트리밍 상태 저장소 (RocksDB)
# =========================================================
# CDC 작업의 안정성을 위해 활성화
spark.sql.streaming.stateStore.providerClass org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider

# =========================================================
# 5. MinIO & S3A 연결 (Desktop IP: 192.168.45.191)
# =========================================================
spark.hadoop.fs.s3a.endpoint                http://192.168.45.191:9000
spark.hadoop.fs.s3a.access.key              minioadmin
spark.hadoop.fs.s3a.secret.key              minioadmin
spark.hadoop.fs.s3a.path.style.access       true
spark.hadoop.fs.s3a.connection.ssl.enabled  false
spark.hadoop.fs.s3a.impl                    org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.metadatastore.impl      org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore

# [편의성] s3:// 로 시작해도 s3a:// 처럼 동작하게 매핑
spark.hadoop.fs.s3.impl                     org.apache.hadoop.fs.s3a.S3AFileSystem

# =========================================================
# 6. Iceberg Catalog
# =========================================================
spark.sql.extensions                                 org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.defaultCatalog                             warehousedev
spark.sql.catalogImplementation                      in-memory

# REST Catalog
spark.sql.catalog.warehousedev                       org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.warehousedev.type                  rest
spark.sql.catalog.warehousedev.uri                   http://192.168.45.191:8181
spark.sql.catalog.warehousedev.io-impl               org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.warehousedev.warehouse             s3://warehousedev
spark.sql.catalog.warehousedev.s3.endpoint           http://192.168.45.191:9000
spark.sql.catalog.warehousedev.s3.path-style-access  true
spark.sql.catalog.warehousedev.s3.region             us-east-1
spark.sql.catalog.warehousedev.s3.access-key-id      minioadmin
spark.sql.catalog.warehousedev.s3.secret-access-key  minioadmin

# =========================================================
# 7. 리소스 관리 (동적 할당)
# =========================================================
spark.dynamicAllocation.enabled                      true
spark.dynamicAllocation.shuffleTracking.enabled      true
spark.dynamicAllocation.minExecutors                 1
# spark.dynamicAllocation.maxExecutors                 5
spark.dynamicAllocation.initialExecutors             1

# =========================================================
# 8. 하드웨어 스펙 안전망
# =========================================================
spark.driver.memory                    2g
spark.driver.cores                     1
spark.executor.memory                  2g
spark.executor.cores                   3
spark.executor.allowMixedArchitectures true

# =========================================================
# Executor 환경변수 주입
# =========================================================
spark.executorEnv.AWS_REGION            us-east-1
spark.executorEnv.AWS_ACCESS_KEY_ID     minioadmin
spark.executorEnv.AWS_SECRET_ACCESS_KEY minioadmin