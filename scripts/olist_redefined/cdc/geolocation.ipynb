{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4c2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414f2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDEFINED_DIR = \"../../../downloads/olist_redefined\"\n",
    "STREAM_DST = Path(os.path.join(REDEFINED_DIR, 'stream'))\n",
    "CDC_DST = Path(os.path.join(REDEFINED_DIR, 'cdc'))\n",
    "os.makedirs(STREAM_DST, exist_ok=True)\n",
    "os.makedirs(CDC_DST, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e35787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000163, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geolocations = pd.read_csv(\"../../../downloads/olist/olist_geolocation_dataset.csv\")\n",
    "geolocationssss = geolocations.drop_duplicates()\n",
    "geolocations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af611df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocations.columns = [col.replace('geolocation_', '') for col in geolocations.columns]\n",
    "geolocations = geolocations.rename(columns={'zip_code_prefix': 'zip_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ebcb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_count_cities_by_zip_code(geolocations: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return the most frequent  and state per ZIP code.\n",
    "    - Normalize Portuguese accents to unify mixed  names.\n",
    "    - Stable tie-breaking (by city name alphabetically).\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original\n",
    "    geo = geolocations.copy()\n",
    "    # Accent normalization (lowercased for consistency)\n",
    "    geo['geolocation_city'] = geo['city'].apply(\n",
    "        lambda x: unidecode(str(x)).lower() if pd.notna(x) else x\n",
    "    )\n",
    "    # Count occurrences per ZIP/city/state\n",
    "    counts = (\n",
    "        geo.groupby(['zip_code', 'city', 'state'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    # Select most frequent per ZIP, stable sort on ties\n",
    "    max_counts = (\n",
    "        counts.sort_values(\n",
    "            ['zip_code', 'count', 'city'],\n",
    "            ascending=[True, False, True]\n",
    "        )\n",
    "        .groupby('zip_code')\n",
    "        .head(1)\n",
    "        [['zip_code', 'city', 'state']]\n",
    "    )\n",
    "    return max_counts\n",
    "\n",
    "def replace_city_state_with_max(geolocations: pd.DataFrame, max_count_cities: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace city/state in geolocation with the most frequent per ZIP code.\n",
    "    \"\"\"\n",
    "    geo = geolocations.drop(columns=['city', 'state']).merge(\n",
    "        max_count_cities,\n",
    "        on='zip_code',\n",
    "        how='left'\n",
    "    )\n",
    "    geo['city'] = geo['city'].fillna('unknown')\n",
    "    geo['state'] = geo['state'].fillna('unknown')\n",
    "    geo['city'] = geo['city'].apply(lambda x: x.lower() if x != 'unknown' else x)\n",
    "    return geo\n",
    "\n",
    "def aggregate_lat_lng(new_geolocations: pd.DataFrame, group, method: str = 'mean') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate latitude and longitude by ZIP/city/state using mean or median.\n",
    "    \"\"\"\n",
    "    agg_func = 'mean' if method == 'mean' else 'median'\n",
    "    return (\n",
    "        new_geolocations.groupby(group)\n",
    "        .agg({\n",
    "            'lat': agg_func,\n",
    "            'lng': agg_func\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "def check_data_integrity(original: pd.DataFrame, final: pd.DataFrame, new_geo: pd.DataFrame,\n",
    "                         lat_range=(-33, 5), lng_range=(-74, -34),\n",
    "                         unique_change_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Check data integrity:\n",
    "    - Unique value preservation threshold\n",
    "    - Null city/state\n",
    "    - Latitude/longitude range validation\n",
    "    - 현업 추가: 0 division 방지, 로그 강화 (Grafana 연계 추천)\n",
    "    \"\"\"\n",
    "    # Unique counts\n",
    "    unique_comparison = pd.DataFrame({\n",
    "        'Metric': ['Zip Code', 'Cities', 'States'],\n",
    "        'Original': [\n",
    "            original['zip_code'].nunique(),\n",
    "            original['city'].nunique(),\n",
    "            original['state'].nunique()\n",
    "        ],\n",
    "        'Final': [\n",
    "            final['zip_code'].nunique(),\n",
    "            final['city'].nunique(),\n",
    "            final['state'].nunique()\n",
    "        ]\n",
    "    })\n",
    "    unique_comparison['Change Ratio'] = (\n",
    "        (unique_comparison['Final'] - unique_comparison['Original']).abs() / \n",
    "        unique_comparison['Original'].replace(0, 1)  # 0 division 방지\n",
    "    )\n",
    "    # Null check\n",
    "    null_city = new_geo['city'].isna().sum()\n",
    "    null_state = new_geo['state'].isna().sum()\n",
    "    # Lat/Lng range check\n",
    "    lat_out = final[\n",
    "        (final['lat'] < lat_range[0]) | (final['lat'] > lat_range[1])\n",
    "    ].shape[0]\n",
    "    lng_out = final[\n",
    "        (final['lng'] < lng_range[0]) | (final['lng'] > lng_range[1])\n",
    "    ].shape[0]\n",
    "    # Reporting (현업: 로그 포맷 강화, Superset 대시보드 연계 추천)\n",
    "    print(\"=== Unique Value Change ===\")\n",
    "    print(unique_comparison.to_string(index=False), \"\\n\")\n",
    "    print(f\"Null count in city: {null_city}\")\n",
    "    print(f\"Null count in state: {null_state}\\n\")\n",
    "    print(f\"Latitude out-of-range rows: {lat_out}\")\n",
    "    print(f\"Longitude out-of-range rows: {lng_out}\\n\")\n",
    "    # Integrity decision\n",
    "    if (unique_comparison['Change Ratio'] > unique_change_threshold).any() or null_city > 0 or null_state > 0 or lat_out > 0 or lng_out > 0:\n",
    "        print(\"[WARNING] Data integrity issue detected. Please correct before saving to Iceberg.\")\n",
    "    else:\n",
    "        print(\"[INFO] Data integrity OK. Safe to proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d555aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unique Value Change ===\n",
      "  Metric  Original  Final  Change Ratio\n",
      "Zip Code     19015  19015      0.000000\n",
      "  Cities      8011   5829      0.272375\n",
      "  States        27     27      0.000000 \n",
      "\n",
      "Null count in city: 0\n",
      "Null count in state: 0\n",
      "\n",
      "Latitude out-of-range rows: 12\n",
      "Longitude out-of-range rows: 10\n",
      "\n",
      "[WARNING] Data integrity issue detected. Please correct before saving to Iceberg.\n"
     ]
    }
   ],
   "source": [
    "# `preprocessed geolocation`\n",
    "\n",
    "# Step 1: Get most frequent city/state per ZIP\n",
    "max_count_cities = get_max_count_cities_by_zip_code(geolocations)\n",
    "# Step 2: Replace original city/state\n",
    "new_geolocations = replace_city_state_with_max(geolocations, max_count_cities)\n",
    "new_geolocations = new_geolocations[['zip_code', 'lat', 'lng', 'state', 'city']]\n",
    "new_geolocations.sort_values(['zip_code', 'state', 'city'], inplace=True)\n",
    "# new_geolocations.to_csv(f\"{BATCH_DST}/geolocation.tsv\", index=False, sep='\\t')\n",
    "# Step 3: Aggregate lat/lng (mean or median)\n",
    "mean_location_zip_code = aggregate_lat_lng(new_geolocations, ['zip_code', 'city', 'state'], method='mean')  # median 추천 가능\n",
    "# Step 4: Data integrity check\n",
    "check_data_integrity(geolocations, mean_location_zip_code, new_geolocations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e954d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `mean geolocation by zip code`\n",
    "# 원본에서 동일 `zip_code`가 여러 개인데, 세부 주소가 없어서 특정할 수 없으므로 평균으로 대체하여 배치 데이터로 사용한다.\n",
    "mean_location_zip_code.sort_values(['zip_code', 'state', 'city'], inplace=True)\n",
    "mean_location_zip_code = mean_location_zip_code[['zip_code', 'lat', 'lng', 'state', 'city']]\n",
    "mean_location_zip_code.to_csv(f\"{CDC_DST}/geolocation.tsv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ab6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `mean geolocations by city`\n",
    "mean_location_city = aggregate_lat_lng(new_geolocations, ['city', 'state'], method='mean')\n",
    "mean_location_city.sort_values(['state', 'city', 'lat', 'lng'], inplace=True)\n",
    "mean_location_city = mean_location_city[['lat', 'lng', 'state', 'city']]\n",
    "# mean_location_city.to_csv(f\"{CDC_DST}/mean_location_city.tsv\", index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-commerce-Ls89Uxei-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
