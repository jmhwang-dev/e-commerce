x-spark-common: &spark-common
  build:
    context: ./infra/spark
    dockerfile: Dockerfile
  image: ecommerce-spark:latest

x-minio-common: &minio-common
  image: minio/minio:RELEASE.2025-06-13T11-33-47Z

services:
  minio:
    container_name: minio
    <<: *minio-common
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - ./data/minio:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 3s
      timeout: 3s
      retries: 10
      start_period: 5s

  init-minio:
    container_name: init-minio
    <<: *minio-common
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./downloads/olist/:/mnt/downloads/olist/
    entrypoint: >
      sh -c '
        mc alias set local http://minio:9000 minioadmin minioadmin &&
        mc mb --ignore-existing local/warehousedev/bronze/checkpoints &&
        mc mb --ignore-existing local/warehousedev/silver/checkpoints &&
        mc mb --ignore-existing local/warehousedev/gold/checkpoints
        # mc cp /mnt/downloads/olist/*.csv local/warehousedev/bronze || true &&
        # echo "init-minio ready; keeping container alive" &&
        # tail -f /dev/null
      '

  spark-client:
    <<: *spark-common
    hostname: spark-client
    container_name: spark-client
    depends_on:
      - spark-master
      - spark-worker
      - spark-history
    ports:
      - "4040:4040"   # application web ui
      - "19080:9080"  # jmx exporter of driver for subscriber bronze to publish silver
      - "19081:9081"  # jmx exporter of driver for subscriber bronze to load bronze
      - "19082:9082"

    volumes:
      - ./configs/spark:/opt/spark/conf
      # - ./configs/:/configs/
      - ./notebooks/spark:/notebooks/
      - ./jobs/:/jobs/
      - ./logs/spark/:/opt/spark/logs/
      - ./downloads/jmx_exporter:/mnt/jmx_exporter/
      - ./configs/jmx/spark_driver.yml:/mnt/configs/jmx/spark_driver.yml

    working_dir: /
    environment:
      - SPARK_NO_DAEMONIZE=true  # run foreground
    
    mem_limit: 4g  # Docker 컨테이너 메모리 제한. JVM 힙메모리는 이것보다 작아야함.
    cpus: 1.0  # 드라이버 코어 제한

    command: >
      /opt/spark/sbin/start-thriftserver.sh

  spark-worker:
    <<: *spark-common
    container_name: spark-worker
    ports:
      - "8084:8081"   # web ui
      - "19083:9080"  # jmx exporter executor for subscriber bronze to publish silver
      - "19084:9081"  # jmx exporter executor for subscriber bronze to load bronze
      - "19085:9082"

    environment:
      SPARK_WORKER_MEMORY: 12g  # upper limit of memory to use
      SPARK_WORKER_CORES: 16    # upper limit of cores to use
    volumes:
      - ./configs/spark:/opt/spark/conf
      - ./downloads/jmx_exporter:/mnt/jmx_exporter/
      - ./configs/jmx/spark_executor.yml:/mnt/configs/jmx/spark_executor.yml

    command: >
      bash -c "
        sleep 10 &&
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "

  spark-master:
      <<: *spark-common
      hostname: spark-master
      container_name: spark-master
      depends_on:
        - spark-history
      ports:
        - "18081:8080"  # web ui: 클러스터 상태 모니터링
        - "7077:7077"   # Spark 클러스터 통신: 클러스터 매니저 <-> 워커노드
      volumes:
        - ./configs/spark:/opt/spark/conf
      command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    
 
  spark-history:
    <<: *spark-common
    container_name: spark-history
    ports:
      - "18080:18080"
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: >
      /bin/bash -c "
        /opt/spark/sbin/start-history-server.sh
      "
    volumes:
      - ./configs/spark:/opt/spark/conf
      - ./logs/spark/:/opt/spark/logs/

  init-spark:
    <<: *spark-common
    container_name: init-spark
    depends_on:
      - spark-master
      - rest-catalog
    volumes:
      - ./configs/spark:/opt/spark/conf
    entrypoint: >
      bash -c "
        echo 'Waiting for master/catalog…';
        sleep 10;
        spark-sql -e \"CREATE NAMESPACE IF NOT EXISTS warehousedev.default;\"
      "
      
  postgres:
    container_name: iceberg-metastore
    image: postgres:16
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes: 
      - "./data/iceberg:/var/lib/postgresql/data"
      - "./infra/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  rest-catalog:
    build:
      context: ./infra/iceberg-rest
      dockerfile: Dockerfile
    container_name: rest-catalog
    depends_on:
      postgres:
        condition: service_healthy
    ports: ["8181:8181"]

    environment:
      CATALOG_URI: jdbc:postgresql://postgres:5432/iceberg
      CATALOG_JDBC_USER: iceberg
      CATALOG_JDBC_PASSWORD: iceberg
      CATALOG_WAREHOUSE: s3://warehousedev/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO

      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: 'true'
      
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1

  inference:
    build:
      context: ./infra/inference
      dockerfile: Dockerfile
    image: ecommerce-inference:latest
    container_name: inference
    command: python inference.py
    # command: bash -c "tail -f /dev/null"
    environment:
      - PYTHONPATH=/mnt/src
    volumes:
      - ./src/:/mnt/src/
      - ./jobs/silver/inference.py:/mnt/app/inference.py
      - ./downloads/models/:/mnt/models/
      - ./logs/:/mnt/logs/
    depends_on:
      schema-registry: { condition: service_healthy }
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # 또는 1, 2 등
              capabilities: [gpu]