{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd15bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/19 11:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from service.utils.spark import get_spark_session\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "spark = get_spark_session(dev=True)\n",
    "test_namespace = 'warehousedev.gold.test'\n",
    "spark.conf.get('spark.sql.catalog.warehousedev.s3.region')\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {test_namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e06cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_iceberg(df, full_table_identifier):\n",
    "    writer = \\\n",
    "        df.writeTo(full_table_identifier)\n",
    "\n",
    "    if not spark.catalog.tableExists(full_table_identifier):\n",
    "        writer.create()\n",
    "    else:\n",
    "        writer.overwritePartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459d6dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 11:28:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# silver\n",
    "order_status_df = spark.read.csv(\"s3a://warehousedev/bronze/tsv/order_status.tsv\", header=True, sep='\\t')\n",
    "\n",
    "delivered_customer_order_id = order_status_df.filter(order_status_df.status == 'delivered_customer').select(['order_id'])\n",
    "delivered_customer_order_df = order_status_df.join(delivered_customer_order_id, on='order_id', how='inner')\n",
    "status_list = ['purchase', 'delivered_customer']\n",
    "delivered_customer_order_df = delivered_customer_order_df.filter(F.col('status').isin(status_list))\n",
    "\n",
    "pivoted_df = delivered_customer_order_df.groupBy('order_id') \\\n",
    "    .pivot('status', ['purchase', 'delivered_customer']) \\\n",
    "    .agg(F.first('timestamp'))\n",
    "\n",
    "delivered_order_df = pivoted_df.withColumnsRenamed({\n",
    "    'purchase': 'purchase_date',\n",
    "    'delivered_customer': 'delivery_date'\n",
    "})\n",
    "    \n",
    "ed_df = spark.read.csv(\"s3a://warehousedev/bronze/tsv/estimated_delivery_date.tsv\", header=True, sep='\\t')\n",
    "delivered_order_df = delivered_order_df.join(ed_df, on='order_id', how='inner')\n",
    "payment_df = spark.read.csv(\"s3a://warehousedev/bronze/tsv/payment.tsv\", header=True, sep='\\t')\n",
    "payment_info_df = payment_df.select(['order_id', 'customer_id']).dropDuplicates()\n",
    "\n",
    "# payment와 order_status에 모두 있는 order_id만 남김\n",
    "clean_order_df = delivered_order_df.join(payment_info_df, on='order_id', how='inner')\n",
    "\n",
    "format_string1 = 'yyyy-MM-dd HH:mm:ss.SSSSSS'\n",
    "format_string2 = 'yyyy-MM-dd HH:mm:ss'\n",
    "clean_order_df = clean_order_df.withColumns({\n",
    "    'purchase_date': F.to_timestamp(F.col('purchase_date'), format_string1),\n",
    "    'delivery_date': F.to_timestamp(F.col('delivery_date'), format_string1),\n",
    "    'estimated_delivery_date': F.to_timestamp(F.col('estimated_delivery_date'), format_string2)\n",
    "    })\n",
    "clean_order_df = clean_order_df.orderBy(F.col('purchase_date').asc())\n",
    "# clean_order_df.show(truncate=False)\n",
    "write_iceberg(clean_order_df, f\"{test_namespace}.clean_order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abb626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d784d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# gold\n",
    "delivery_stats = clean_order_df \\\n",
    "    .withColumn(\n",
    "        'delivery_lead_time',\n",
    "        F.datediff(F.col('delivery_date'), F.col('purchase_date'))) \\\n",
    "    .withColumn(\n",
    "        'is_late',\n",
    "        F.when(F.col('delivery_date') <= F.col('estimated_delivery_date'), False)\n",
    "        .otherwise(True)\n",
    "    )\n",
    "# delivery_stats.show()\n",
    "write_iceberg(delivery_stats, f\"{test_namespace}.delivery_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d73943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# silver\n",
    "order_item_df = spark.read.csv(\"s3a://warehousedev/bronze/tsv/order_item.tsv\", header=True, sep='\\t')\n",
    "product_df = spark.read.csv(\"s3a://warehousedev/bronze/tsv/product.tsv\", header=True, sep='\\t')\n",
    "\n",
    "taget_category = 'health_beauty'\n",
    "taget_category_product = product_df.filter(product_df.category == taget_category).select(['product_id', 'category'])\n",
    "taget_category_order = taget_category_product.join(order_item_df, on='product_id', how='inner')\n",
    "taget_category_order = taget_category_order.sort('product_id')\n",
    "\n",
    "# 하나의 order에 여러 item이 있을 수 있으므로, order_item_id는 남겨둔다.\n",
    "# `total_price` 는 상품가와 배송비를 합산한 가격\n",
    "taget_category_order = taget_category_order.withColumn('total_price', F.round(F.col('price') + F.col('freight_value'), 4))\n",
    "taget_category_order = taget_category_order.drop('shipping_limit_date', 'price', 'freight_value')\n",
    "clean_category_order_df = taget_category_order.join(clean_order_df.select(['order_id']), on='order_id', how='inner')\n",
    "# clean_category_order_df.show(truncate=False)\n",
    "write_iceberg(clean_category_order_df, f\"{test_namespace}.clean_category_order\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7526fb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# gold\n",
    "sale_stats = clean_category_order_df.groupBy('product_id').agg(\n",
    "    F.count('order_id').alias('order_count'),\n",
    "    F.round(F.sum(F.col('total_price')), 4).alias('total_sales')\n",
    "    ).orderBy(F.col('order_count').desc())\n",
    "\n",
    "sale_stats = sale_stats.withColumn('mean_sale', F.round(F.col('total_sales') / F.col('order_count'), 4))\n",
    "# sale_stats.show(truncate=False)\n",
    "write_iceberg(sale_stats, f\"{test_namespace}.sale_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28195405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# silver: review metadata\n",
    "review_df = spark.read.csv(\"s3a://warehousedev/bronze/tsv/review.tsv\", header=True, sep='\\t')\n",
    "\n",
    "review_metadat_df = review_df.drop('review_comment_title', 'review_comment_message')\n",
    "clean_review_metadata_df = review_metadat_df.join(clean_category_order_df.select('order_id', 'product_id'), on=['order_id'], how='inner')\n",
    "clean_review_metadata_df = clean_review_metadata_df.orderBy('product_id')\n",
    "\n",
    "format_string = \"yyyy-MM-dd HH:mm:ss\"\n",
    "clean_review_metadata_df = clean_review_metadata_df \\\n",
    "    .withColumn('review_creation_date', F.to_timestamp(F.col('review_creation_date'), format_string)) \\\n",
    "    .withColumn('review_answer_timestamp', F.to_timestamp(F.col('review_answer_timestamp'), format_string))\n",
    "\n",
    "clean_review_metadata_df = clean_review_metadata_df.withColumn('answer_lead_time', F.datediff(F.col('review_answer_timestamp'), F.col('review_creation_date')) )\n",
    "\n",
    "clean_review_metadata_df = clean_review_metadata_df.drop('review_creation_date', 'review_answer_timestamp')\n",
    "# clean_review_metadata_df.show()\n",
    "write_iceberg(clean_review_metadata_df, f\"{test_namespace}.clean_review_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e6fcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geolocation\n",
      "customer\n",
      "seller\n"
     ]
    }
   ],
   "source": [
    "for file_name in ['geolocation', 'customer', 'seller']:\n",
    "    print(file_name)\n",
    "    tmp_df = spark.read.csv(f\"s3a://warehousedev/bronze/tsv/{file_name}.tsv\", header=True, sep='\\t')\n",
    "    write_iceberg(tmp_df, f\"{test_namespace}.{file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
